<!DOCTYPE html>
<html lang="en">

  <head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>CS205 Real Time Image Stitching</title>

    <!-- Bootstrap core CSS -->
    <link href="https://fonts.googleapis.com/css?family=Montserrat" rel="stylesheet">
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/scrolling-nav.css" rel="stylesheet">
    <link href="css/style.css" rel="stylesheet">
    <link rel="shortcut icon" type="image/x-icon" href="img/seas-logo.ico"/>

  </head>

  <body id="page-top">

    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top" id="mainNav">
      <div class="container">
        <a class="navbar-brand js-scroll-trigger" href="#page-top">Real-Time Image Stitching</a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
          <ul class="navbar-nav ml-auto">
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#overview">Introduction</a>
            </li>
            <br>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#eda">Design Approach</a>
            </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#follower">Performance Results</a>
            </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#playlist">Discussions</a>
            </li>
          </ul>
        </div>
      </div>
    </nav>

    <header class="text-white">
      <div class="container text-center">
        <h1>Real-Time Image Stitching</h1>
        <p class="lead">CS205 Computing Foundations for Computational Science Final Project</p>
        <p id="info">Group 4: Weihang Zhang, Xuefeng Peng, Jiacheng Shi and Ziqi Guo</p>
        <p id="info">Harvard University, Spring 2018</p>
      </div>
    </header>

    <section id="overview">
      <div class="container">
        <div class="row">
          <div class="col-lg-8 mx-auto">
            <h1>Introduction</h1><br>
            <h4>Problem Statement</h4>
            <br>
            <p>In this project, we want to use big compute techniques to parallelize the algorithms of <strong>image stitching</strong>, so that we can stream videos from adjascent camera into a single panoramic view.</p>
            <p>Image stitching or photo stitching is the process of combining multiple photographic images with overlapping fields of view to produce a segmented panorama or high-resolution image (example below).</p>
            <br><br>
            <p class="img"><img src="img/stitching_example.jpg" height="300" width="750"></p>
            <br><br>
            <h4>Procedures of Image Stitching</h4>
            <br>
            <p>Image stitching is a rather complicated application in computer vision. It is composed of several key stages, some of which involve heavy computation. We will give an intuitive explanation of each of the key procedures below. Please follow the links provided for more technical details, and please refer to the Design Approach section for a complexity profiling of these tasks.</p>
            <ol>
              <li><strong>Keypoint Detection and Description</strong></li><br>
              <p>As a first step, keypoints on the two images you want to stitch together need to be identified. These keypoints usually correspond to the most distinguish features of an image, such as corners and edges of an object. There are several famous algorithms that dedicately achieve this task, including Harris Corner Detection, Scale Invariant Feature Transform (SIFT) and Speed-Up Robust Features (SURF). SIFT and SURF are the state-of-the-art due to their robustness.</p>
              <p>Not only do these algorithms identify the keypoints, they will also generate a descriptor vector for each of the keypoints. These descriptors will capture information about the keypoints' location, orientation and relation to their surroundings.</p>
              <li><strong>Keypoint Matching</strong></li><br>
              <p>After keypoint detection, each image will have a set of keypoints together with their descriptor vectors. We will then need to establish matching relations between these descriptors. Most methods are based on Euclidean distance between descriptors. For each keypoint in image 1, if its best match is significantly better than the second best match, than we consider the best match valid.</p>
              <li><strong>Transformation Estimation</strong></li>
              <p>Once we have establish matching keypoints between images, we want to estimate a transformation matrix H that will be used to warp the image. Normally, Random Sample Consensus (RANSAC) will be used to derive reliable transformation matrix after removing false matches. The algorithm basically iteratively try out different match combinations and only keep the one that is the best-fitting to the matches.</p>
              <li><strong>Warping</strong></li>
              <p>With the transformation matrix, we can project the image to the right to the plane that the image to the left is at. This is called warping.</p>
              <li><strong>Stitching</strong></li>
              <p>Finally, we have the original image 1, and the warped image 2. We can stitch them together by placing pixels from both images on a blank canvas.</p>
              <li><strong>Border Blending</strong></li>
              <p>Border blending is to smooth out the differences in light and hue across the stitching seam, so that the stitched image can look more homogeneous.</p>
            </ol>
            <br><br>
            <h4>Need for High Performance Computing</h4>
            <p>The key idea of our project is to focus on the word ‘real-time’. Although there are existing algorithms for the application, doing it in ‘real-time’ can still be challenging.</p>
            <p>Imagine the scenario where we need a real-time panorama streaming view, this would require the backend application to stitch the view of different cameras together frame by frame. For a laggy video of 5 frames per second, we will need processing time of at most 0.2 seconds for each pair of images. This would be very difficult to achieve with regular computational resources. With a faster stitching process of one pair of images, we can process more image pairs per second, which would result in a higher Frames Per Second (FPS). FPS is a direct measure of how fluid the video looks to human eyes.</p>
            <p>In addition, with high performance computing, we can stitch images of higher resolution, at a visually fluid speed. This is the whole motivation of parallelizing the task of image stitching.</p>


            <br><br>
            <h4>Challenges</h4>
            <p>There are several challenges associated with our project:</p>
            <ul>
              <li>Some algorithm such as SIFT or SURF is rather complicated to understand as well as to re-implement. They are not composed solely of independent loops, but have many data dependencies that need to be dismantled before parallelization.</li>
              <li>The desirable speedup might be challenging to achieve. The sequential version will take only a few seconds to run once. This means overheads such as memory access, synchronization and communication are by no means negligible. Speeding it up to achieve multiple frames per second entails careful optimization of overheads.</li>
              <li>The program might be heavily memory bound, as a high resolution image will have millions of pixels, and we can potentially find a large amount of keypoints. Accessing and communicating the original images and these keypoints together with their descriptors will add to the execution time.</li>
              <li>Our parallelization target is not a single algorithm. It is a sequence of algorithms achieving different tasks. We need to consider the possibility of task-level scheduling and parallelization of these different tasks.</li>
            </ul>
            <p></p>



          </div>
        </div>
      </div>
    </section>

    <section id="eda" class="bg-light">
      <div class="container">
        <div class="row">
          <div class="col-lg-8 mx-auto">
            <h1>Design Approach</h1><br>
            <h4>Code and Data Source</h4>
            <p>We base our parallelization based on a sequential implementation of keypoint detection using Speed-Up Robust Features (SURF) as well as matching. The code can be found at <a href="https://github.com/abhinavgupta/SURF">https://github.com/abhinavgupta/SURF</a>.</p>
            <p>To finish up the whole stitching process, we implemented our own transformation estimation, warping and stitching algorithm. For transformation estimation, we relied on OpenCV's implementation as this step is not one of the computationally heaviest ones. For warping and stitching, we implemented them without using any vision libraries so that we can have full control of the parallelization.</p>
            <p>As for data, we shot images and videos on 2 <a href="https://www.amazon.com/Logitech-Laptop-Webcam-Design-360-Degree/dp/B004YW7WCY/ref=sr_1_8?s=pc&ie=UTF8&qid=1525394553&sr=1-8&keywords=logitech+webcam">Logitech HD Laptop Webcam C615</a>. It is capable of shooting up to 1080p video.</p>

            <br><br>

            <h4>Profiling</h4>
            <p>We run the sequential code of stitching on two images of different resolutions and plot the execution time of each stage. We omitted transformation estimation, as OpenCV has a very efficient implementation of it, which takes negligible time compared to the other stages.</p>
            <p class="img"><img src="img/profile.png"></p>
            <p>With the larger image, keypoint detection, keypoint description and keypoint matching are the three most time-consuming steps. With the smaller image, the running time of matching decreases so much that it even takes shorter than warping. This is expected, as when the image is larger, there will be exponentially more interest points, which takes exponentially more time to match. Given that the maximum video resolution we are targeting is 1080p, which is smaller than the small image, we shouldn't be concerned too much about parallelizing it. As for stitching, it is clearly negligible compared to the other steps.</p>
            <p>From this profiling, it tells us we should be more concerned about <strong>keypoint detection</strong>, <strong>keypoint description</strong> and <strong>warping</strong>. </p>
            <br><br>

            <h4>Parallelization Implementation</h4>
            <br>
            <h5>Parallelization Targets</h5>
            <p>Based on the profiling results shown above, we can see that <strong>keypoint detection</strong>, <strong>descriptor extraction</strong> and <strong>warping</strong> are the computationally heaviest and thus should be parallelized the most.</p>
            <br>
            <h5>Technologies</h5>
            <ul>
              <li><strong>Programming language</strong>: C++</li>
              <li><strong>Tools: </strong>OpenACC, OpenMP, std::thread</li>
            </ul>
            <br>
            <h5>Programming Models</h5>
            <p>We will use several parallel computing paradigms and compare their performance:</p>
            <p>
              <li><strong>Shared Memory Parallelization (OpenMP)</strong></li>
              <p>For shared memory parallelization, we just need to add directives to independent loops. Shared variables and private variables need to be differentiated.
              <li><strong>GPU Acceleration (OpenACC)</strong></li>
              <p>GPU acceleration is very suitable for this type of task. The many cores in GPU provide fundamentally larger computing power. However, we need to pay special attention to the following aspects:</p>
              <ul>
                <li>Many loops are dealing with image or matrix variables that take up much memory. Redundant memory copies will create huge overhead that might just negate the speedup from parallelization. We need to limit memory copies as much as possible by copying in variables before parallel regions.</li>
                <li>OpenACC currently does not support many special data structures, for example the mat object in OpenCV or IplImage. In order to parallelize regions where these data structures are used, we rewrote these parts with the most common data structures in c++.</li>
                <li>For variables that have different values in each iteration, we need to define those as private variables to ensure code integrity.</li>
              </ul>
              <li><strong>Advanced Feature: Task-Level Parallelization (Threading)</strong></li>
              <p>The whole task of image stitching can be broken down to various stages. In addition to the algorithms mentioned previously, we still need to extract frames from webcam video stream, and output video from stitched images. </p>
              <p>In the sequential version, after one task is finished, it needs to wait for all the downstream steps to be completed in order to be restarted. In this case, the critical path is all the stages of the entire workflow.</p>
              <p class="img"><img src="img/sequential.png" height="350" width="250"></p>

              <p>By pipelining the different stages with the multiple CPU threads available, we can achieve task-level parallelization so that idle time is reduced. Each thread notify their downstream when they finish their current iteration and jump right into the next. In this way, the total execution time will be constrained by the bottleneck of the four threads. We designed our pipeline to be the following diagram:</p>
              <p class="img"><img src="img/pipeline.png" height="330" width="750"></p>
              <p>The pipeline utilizes four threads that focus on different tasks. The main thread has to render images due to the constraint of OpenCV. The main thread will continuously render images from a mutex object that contains the stitched images, which ensure the data's integrity. The other threads communicate by notifying each other when one iteration is completed.</p>

              <li><strong>Hybrid Parallelization</strong></li>
              <p>To achieve maximum speedup, we can integrate task-level parallelization with procedure/loop level parallelization.</p>
              <p>The way we designed our task-level pipeline allows for integration with GPU acceleration. OpenACC only allows to be called from only one thread. In our pipeline, the stitching thread is the only thread that needs to do heavy computation. Therefore, we can accelerate the stitching thread by letting it operate on GPU. The other threads such as image i/o are less compute-intensive, and thus will be just fine operating on CPU. </p>
            <br>
            <h5>Parallelism Taxonomy</h5><br>
            <p class="img"><img src="img/taxonomy.png" height="180" width="650"></p>

          </div>
        </div>
      </div>
    </section>

    <section id="follower">
      <div class="container">
        <div class="row">
          <div class="col-lg-8 mx-auto">
            <h1>Performance Results</h1><br>
            <h4>Evaluation Approach</h4>
            <p>To evaluate the performance of our parallelized program, we will use videos captured at different resolutions. The higher the resolution, the more computationally heavy it is to stitch the videos. We will use the average execution time of each frame to calculate speedup, and use the FPS as a measure of throughput.</p>
            <p>As for testing platform, we will use the <strong>AWS EC2 t2.2xlarge instance</strong> for baseline and shared memory parallel program. For GPU involved programs, we will evaluate the performance on two devices, a <strong>local PC</strong> with GPU and an <strong>AWS EC2 g3.4xlarge</strong> instance. The specifications of the local PC in comparison to the AWS instance are tabulated below:</p>
            <table class="table table-hover table-striped">
              <caption>Table 1. Device specifications for GPU programs</caption>
              <thead>
              <tr>
                <th scope="col">Variable Name</th>
                <th scope="col">Local PC</th>
                <th scope="col">AWS g3.4xlarge</th>
              </tr>
              </thead>
              <tbody>
              <tr>
                <th scope="row">Operating system</th>
                <td>Ubuntu 16.04</td>
                <td>Ubuntu 16.04</td>
              </tr>
              <tr>
                <th scope="row">CPU model</th>
                <td>Intel(R) Xeon(R) CPU E3-1231 v3 @ 3.40GHz</td>
                <td>Intel(R) Xeon(R) CPU E5-2676 v3 @ 2.40GHz</td>
              </tr>
              <tr>
                <th scope="row">vCPU(s)</th>
                <td>8</td>
                <td>8</td>
              </tr>
              <tr>
                <th scope="row">GPU model</th>
                <td>NVIDIA GeForce GTX 1070</td>
                <td>NVIDIA Tesla M60</td>
              </tr>
              <tr>
                <th scope="row">GPU architecture</th>
                <td>Pascal</td>
                <td>Maxwell 2.0</td>
              </tr>
              <tr>
                <th scope="row">Core clock / boost Clock (MHz)</th>
                <td>1506/1683</td>
                <td>930/1180</td>
              </tr>
              <tr>
                <th scope="row">Floating-point performance (gflops)</th>
                <td>6463</td>
                <td>4833</td>
              </tr>
              </tbody>
            </table>
            <p>All used codes and test cases are documented with detailed instructions, which can be found at our <a href="https://github.com/ziqiguo/CS205-ImageStitching">Github repository</a>.</p>
            <br><br>
            <h4>Serial Version</h4>
            <p>In this section, we see the performance results of the serial version as a baseline.</p>
            <p>First let's see the execution time of each stage in the stitching process for videos of different resolutions. We only include stages that involve heavier computation.</p>
            <table class="table table-hover table-striped">
              <caption>Table 2. Execution time of serial version</caption>
              <thead>
              <tr>
                <th scope="col">Resolution</th>
                <th scope="col">480p</th>
                <th scope="col">720p</th>
                <th scope="col">1080p</th>
              </tr>
              </thead>
              <tbody>
              <tr>
                <th scope="row">Keypoint detection</th>
                <td>0.162</td>
                <td>0.420</td>
                <td>0.954</td>
              </tr>
              <tr>
                <th scope="row">Keypoint description</th>
                <td>0.067</td>
                <td>0.141</td>
                <td>0.226</td>
              </tr>
              <tr>
                <th scope="row">Keypoint matching</th>
                <td>0.007</td>
                <td>0.030</td>
                <td>0.070</td>
              </tr>
              <tr>
                <th scope="row">Homography</th>
                <td>0.005</td>
                <td>0.007</td>
                <td>0.046</td>
              </tr>
              <tr>
                <th scope="row">Warping</th>
                <td>0.026</td>
                <td>0.064</td>
                <td>0.153</td>
              </tr>
              <tr>
                <th scope="row">Stitching</th>
                <td>0.001</td>
                <td>0.002</td>
                <td>0.014</td>
              </tr>
              <tr>
                <th scope="row">Total</th>
                <td>0.304</td>
                <td>0.693</td>
                <td>1.527</td>
              </tr>
              </tbody>
            </table>
            <div id="seq-time"></div>
            <p>Then let's see the how Frames Per Second varies for videos of different resolutions.</p>
            <table class="table table-hover table-striped">
              <caption>Table 2. Execution time of serial version</caption>
              <thead>
              <tr>
                <th scope="col">Resolution</th>
                <th scope="col">480p</th>
                <th scope="col">720p</th>
                <th scope="col">1080p</th>
              </tr>
              </thead>
              <tbody>
              <tr>
                <th scope="row">FPS</th>
                <td>3.3</td>
                <td>1.4</td>
                <td>0.7</td>
              </tr>
              </tbody>
            </table>
            <h4>OpenMP Version</h4>
            <p>In this section, we show the performance results after implementing shared memory parallelization with OpenMP.</p>
            <h4>OpenACC Version</h4>
            <p>In this section, we show the performance results after implementing GPU acceleration with OpenACC.</p>
            <table class="table table-hover table-striped">
              <caption>Table 2. Execution time of serial version</caption>
              <thead>
              <tr>
                <th scope="col">Platform</th>
                <th scope="col" colspan="3">AWS g3.4xlarge</th>
                <th scope="col" colspan="3">GTX 1070</th>
              </tr>
              <tr>
                <th scope="col">Resolution</th>
                <th scope="col">480p</th>
                <th scope="col">720p</th>
                <th scope="col">1080p</th>
                <th scope="col">480p</th>
                <th scope="col">720p</th>
                <th scope="col">1080p</th>
              </tr>
              </thead>
              <tbody>
              <tr>
                <th scope="row">Keypoint detection</th>
                <td>0.016</td>
                <td>0.033</td>
                <td>0.073</td>
                <td>0.011</td>
                <td>0.027</td>
                <td>0.061</td>
              </tr>
              <tr>
                <th scope="row">Keypoint description</th>
                <td>0.020</td>
                <td>0.024</td>
                <td>0.030</td>
                <td>0.010</td>
                <td>0.014</td>
                <td>0.018</td>
              </tr>
              <tr>
                <th scope="row">Keypoint matching</th>
                <td>0.003</td>
                <td>0.004</td>
                <td>0.009</td>
                <td>0.0007</td>
                <td>0.003</td>
                <td>0.006</td>
              </tr>
              <tr>
                <th scope="row">Homography</th>
                <td>0.003</td>
                <td>0.006</td>
                <td>0.024</td>
                <td>0.009</td>
                <td>0.009</td>
                <td>0.034</td>
              </tr>
              <tr>
                <th scope="row">Warping</th>
                <td>0.005</td>
                <td>0.011</td>
                <td>0.026</td>
                <td>0.003</td>
                <td>0.008</td>
                <td>0.023</td>
              </tr>
              <tr>
                <th scope="row">Stitching</th>
                <td>0.002</td>
                <td>0.0025</td>
                <td>0.007</td>
                <td>0.0007</td>
                <td>0.001</td>
                <td>0.013</td>
              </tr>
              <tr>
                <th scope="row">Total</th>
                <td>0.055</td>
                <td>0.125</td>
                <td>0.267</td>
                <td>0.048</td>
                <td>0.109</td>
                <td>0.264</td>
              </tr>
              </tbody>
            </table>

            <div id="acc-tesla-speedup"></div>
            <div id="acc-gtx-speedup"></div>

            <table class="table table-hover table-striped">
              <caption>Table 2. Execution time of serial version</caption>
              <thead>
              <tr>
                <th scope="col">Platform</th>
                <th scope="col" colspan="3">AWS g3.4xlarge</th>
                <th scope="col" colspan="3">GTX 1070</th>
              </tr>
              <tr>
                <th scope="col">Resolution</th>
                <th scope="col">480p</th>
                <th scope="col">720p</th>
                <th scope="col">1080p</th>
                <th scope="col">480p</th>
                <th scope="col">720p</th>
                <th scope="col">1080p</th>
              </tr>
              </thead>
              <tbody>
              <tr>
                <th scope="row">FPS</th>
                <td>18</td>
                <td>8</td>
                <td>3.74</td>
                <td>20</td>
                <td>9</td>
                <td>3.8</td>
              </tr>
              </tbody>
            </table>
            <h4>Pipeline Version</h4>
            <p>In this section, we show the performance results after implementing task-level parallelization with our multi-threading pipeline, on top of GPU acceleration.</p>
            <table class="table table-hover table-striped">
              <caption>Table 2. Execution time of serial version</caption>
              <thead>
              <tr>
                <th scope="col">Resolution</th>
                <th scope="col">480p</th>
                <th scope="col">720p</th>
                <th scope="col">1080p</th>
              </tr>
              </thead>
              <tbody>
              <tr>
                <th scope="row">FPS</th>
                <td>35</td>
                <td>14</td>
                <td>5</td>
              </tr>
              </tbody>
            </table>
            <h4>Comparison</h4>
            <p>In this section, we compare the speedup of all parallel versions against the serial version.</p>
            <div id="fps-comparison"></div>
          </div>
        </div>
      </div>
    </section>

    <section id="playlist" class="bg-light">
      <div class="container">
        <div class="row">
          <div class="col-lg-8 mx-auto">
            <h1>Discussions</h1><br>
            <h4>Insights</h4>
            <p>Through this project, we conquered quite a few obstacles and obtained some precious insights from the process:</p>
            <ul>
              <li>With sufficient optimization, GPU acceleration is very suitable for image stitching. Sub-procedures like keypoint detection, keypoint description and perspective warping all involve repetitive loop operations that benefit tremendously from GPU's computing prowess.</li>
              <li>We managed to control overhead in GPU memory copies, by reimplementing the code such that one memory copy before the parallel region is sufficient.</li>
              <li>Through the simple idea of using a moving average of homography, video stability is significantly improved without utilizing any complicated stabilization algorithms.</li>
              <li>We managed to design a task-level pipeline that exploits the computing resources from both CPU and GPU. It also balances workload to a great extent. This paradigm is extensively useful, as just by initializing additional threads, it can handle additional operations that can improve the stitching results such as bundle adjustment, without incurring too much additional execution time.</li>
              <li>The success of this application tells the possibility of making a lot of other computer vision tasks real-time. Sub-procedures such as keypoint detection, perspective warping can be transplanted in a lot of other important applications, such as video stabilization, object contouring and object straightening. </li>
            </ul>
            <br><br>
            <h4>Future Work</h4>
            <p>There are of course some areas for improvement that call for future endeavors. For example, although we do not favor the idea of distributed memory parallelization with MPI due to the amount of data that have to be communicated, it might still be worth a try.</p>
            <p>In addition, a lateral comparison of the scalability of different keypoint detection algorithms (SIFT, SURF, corner detection) might be interesting. Possibly more advanced operations like bundle adjustment can be assigned to the rest of the CPU threads available.</p>


          </div>
        </div>
      </div>
    </section>


    <!-- Footer -->
    <footer class="py-5 bg-dark">
      <div class="container">
        <p class="m-0 text-center text-white">Copyright &copy; Harvard CS205 2018 Spring Group 7</p>
        <p class="m-0 text-center text-white">A project by <a href="https://www.linkedin.com/in/jiacshi/">Jiacheng Shi</a>, <a href="https://www.linkedin.com/in/tonyzhanguiuc/">Weihang Zhang</a>, <a href="https://www.linkedin.com/in/xuefengpeng/">Xuefeng Peng</a> and <a href="https://www.linkedin.com/in/ziqi-guo-974aa69a/">Ziqi Guo</a></p>
        <p class="m-0 text-center text-white">Visit <a href="https://github.com/ziqiguo/CS205-ImageStitching">Github Repository</a></p>
      </div>
      <!-- /.container -->
    </footer>

    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>


    <!-- Plugin JavaScript -->
    <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

    <!-- Plotting Library -->
    <script src="vendor/plotly-latest.min.js"></script>

    <!-- Custom JavaScript for this theme -->
    <script src="js/scrolling-nav.js"></script>
    <script src="js/plots.js"></script>


  </body>

</html>
